#!/bin/bash

echo "ğŸš€ Setting up Ollama for Free AI Crop Recommendations"
echo "=================================================="

# Check if Ollama is already installed
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama is already installed"
else
    echo "ğŸ“¥ Installing Ollama..."
    curl -fsSL https://ollama.ai/install.sh | sh
fi

# Start Ollama service
echo "ğŸ”„ Starting Ollama service..."
ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to start
echo "â³ Waiting for Ollama to start..."
sleep 10

# Download Llama2 model (free and good for crop recommendations)
echo "ğŸ“¥ Downloading Llama2 model (this may take a few minutes)..."
ollama pull llama2

# Download Mistral model (alternative, smaller and faster)
echo "ğŸ“¥ Downloading Mistral model (alternative)..."
ollama pull mistral

# Test the installation
echo "ğŸ§ª Testing Ollama installation..."
if ollama list | grep -q "llama2"; then
    echo "âœ… Llama2 model installed successfully"
else
    echo "âŒ Failed to install Llama2 model"
    exit 1
fi

echo ""
echo "ğŸ‰ Ollama setup complete!"
echo ""
echo "ğŸ“‹ Next steps:"
echo "1. Copy ai-service/env.example to ai-service/.env"
echo "2. Run: cd ai-service && npm install"
echo "3. Run: npm start"
echo ""
echo "ğŸ”§ Configuration:"
echo "- Ollama URL: http://localhost:11434"
echo "- Default Model: llama2"
echo "- Alternative Model: mistral"
echo ""
echo "ğŸ’¡ Tips:"
echo "- Ollama runs locally on your machine"
echo "- No API keys required"
echo "- Completely free to use"
echo "- Models are downloaded once and cached locally"
echo ""
echo "ğŸ›‘ To stop Ollama: kill $OLLAMA_PID"
echo "ğŸ”„ To restart Ollama: ollama serve"
